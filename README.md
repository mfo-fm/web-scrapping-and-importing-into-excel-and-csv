# web-scrapping-and-importing-into-excel-and-csv
scraping a quote website, then exporting the quotes into excel and csv files


So this is a more advanced scrapping of http://quotes.toscrape.com, where the data ingested from the website is kept in a datafram then imported into an excel file and a csv file.

This is achieved using the libraries CSV for handling csv files and PANDAS for handling excel files, the codes are achieved accordingly.

WEBSCRAPPING OF QUOTES INTO CSV/EXCEL: this only scrapes the page it is pointed to, in this case it's the first page of the website, after importing necessary libraries i defined the target URL, then send a get request to the URL and check if the response is successful, if the response is successful, it parses the HTML content of the webpage using BeautifulSoup, finds all the quote elements on the page using the specified CSS class, then opens a CSV or an excel file in write mode and creates a DictWriter object to write data to the CSV or Excel file, writes the column headers (fields) to the CSV or Excel file, iterates over each quote, extracts the text, author, and tags, and writes them to the file, finally prints a success message if quotes are found and exported to the file, otherwise prints a message indicating no quotes were found, if the response status code is not 200, it prints a message indicating the failure to retrieve data from the website.

WEBSCRAPPING 20 PAGES QUOTES INTO CSV/EXCEL: this follows the same step as the previous, just that here we have a specific number of pages that I want to scrape and the number of quotes per page, I import necessary libraries, then define variables for the base URL, page number, maximum pages to scrape, and number of quotes per page,an empty list is initiated to store all the quotes,executes a while loop to iterate through each page within the specified range of maximum pages,constructs the URL for each page,then sends a GET request to the URL and checks if the response is successful,parses the HTML content using BeautifulSoup and extracts quotes, authors, and tags, appends the extracted data to the list of all quotes,if quotes are found, converts the list of quotes into a DataFrame,writes the DataFrame to an Excel or CSV file named "quotes.xlsx" or "quotes.csv",prints a success message if quotes are found and exported to the file, otherwise prints a message indicating no quotes were found.

WEBSCRAPING 20 PAGES QUOTES INTO BOTH CSV AND EXCEL FILES: so what I am trying to achieve here is to write a code that imports all quotes directly to both csv and excel files inside the same code, let me give a concise summary> I import necessary libraries,defines variables for the base URL, page number, maximum pages to scrape, and number of quotes per page,initiates an empty list to store all the quotes,executes a while loop to iterate through each page within the specified range of maximum pages,constructs the URL for each page,sends a GET request to the URL and checks if the response is successful,parses the HTML content using BeautifulSoup and extracts quotes, authors, and tags,appends the extracted data to the list of all quotes,if quotes are found, exports the list of quotes to a CSV file and an Excel file,prints a success message if quotes are found and exported to the files, otherwise prints a message indicating no quotes were found.
